# kafka_to_sparkstreaming_to_graph

This Spark application loads data from a Kafka queue into a DSE Graph instance, by way of Spark Streaming and DSE GraphFrames APIs.


== RELEVANT DOCUMENTATION

* Apache Kafka: http://kafka.apache.org/documentation/

* Apache Spark Streaming: https://spark.apache.org/docs/latest/streaming-programming-guide.html

* Kafka + Spark Streaming Integration: https://spark.apache.org/docs/latest/streaming-kafka-integration.html

* DSE GraphFrames: https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/graph/graphAnalytics/dseGraphFrameImport.html


== BUILD INSTRUCTIONS

This project leverage sbt for Scala package management and compilation. In addition, one must have DSE Spark already running on their cluster. Make sure sbt is installed on your machine (https://www.scala-sbt.org/1.0/docs/Installing-sbt-on-Linux.html):

```
echo "deb https://dl.bintray.com/sbt/debian /" | sudo tee -a /etc/apt/sources.list.d/sbt.list
sudo apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 2EE0EA64E40A89B84B2DF73499E82A75642AC823
sudo apt-get update
sudo apt-get install sbt

```

Next, clone the `kafka_to_sparkstreaming_to_graph` project to your cluster that is running DSE Spark, and build with sbt

```
git clone git@github.com:ShaunakDas88/kafka_to_sparkstreaming_to_graph.git
cd <PATH TO kafka_to_sparkstreaming_to_graph>
sbt package
```



== ABOUT THE GRAPH DATA SET

In this Spark application, we will be dealing with a subset of the Amazon graph data set (http://jmcauley.ucsd.edu/data/amazon/links.html) to your DSE Spark cluster. The data comes in the following JSON format:


which makes the DataFrames API a natural choice to use for its storage and manipulation in Spark.

Provided in the project is a bash script for downloading
```
wget -P <DESTINATION DIRECTORY>
wget -P <DESTINATION DIRECTORY>
```


== APACHE KAFKA SETUP

This Spark application will be reading data that is streamed into appropriate queues (topics) in Kafka. The following steps will get you started with this initial setup:

* From the project's root directory unpack the Kafka project that is provided:
```
tar -zxvf resources/kafka_2.11-0.11.0.1.tgz
```
You should now see a subdirectory `kafka_2.11-0.11.0.1`, which contains pre-built Kafka 2.11. 

```
~/kafka_to_sparkstreaming_to_graph$ ls
build.sbt  kafka_2.11-0.11.0.1  kafka_2.11-0.11.0.1.tgz  README.md  resources  scripts  src
```


* Launch the ZooKeeper server as a background process:
```
~/kafka_to_sparkstreaming_to_graph$ kafka_2.11-0.11.0.1/bin/zookeeper-server-start.sh kafka_2.11-0.11.0.1/config/zookeeper.properties &
```
Note that we are specifying an already provided configuration file `kafka_2.11-0.11.0.1/config/zookeeper.properties` here. 

When listing all running Java processes, we should now see `QuorumPeerMain`
```
~/kafka_to_sparkstreaming_to_graph$ jps
14417 QuorumPeerMain
14703 Jps
```

* Launch the Kafka server as a background process:
```
~/kafka_to_sparkstreaming_to_graph$ kafka_2.11-0.11.0.1/bin/kafka-server-start.sh kafka_2.11-0.11.0.1/config/server.properties &
```
Again, we are using a provided configuration file `kafka_2.11-0.11.0.1/bin/kafka-server-start.sh kafka_2.11-0.11.0.1/config/server.properties` for this server. 

When listing all running Java processes, we should now also see `Kafka`:
```
~/kafka_to_sparkstreaming_to_graph$ jps
14417 QuorumPeerMain
15396 Jps
15071 Kafka
```

=== LOADING THE DATA INTO KAFKA TOPICS

The main abstraction which Kafka uses for reading and storing streamed input is a topic. Our Amazon data set consists of two json files:

-
- 

Let's go ahead and make a topic for each of these files:

```
~/kafka_to_sparkstreaming_to_graph$ kafka_2.11-0.11.0.1/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1  --topic metadata


~/kafka_to_sparkstreaming_to_graph$ kafka_2.11-0.11.0.1/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1  --topic reviews
```

Let us verify that the topics were in fact successfully created:
```
~/kafka_to_sparkstreaming_to_graph$ kafka_2.11-0.11.0.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181
[2018-07-02 00:19:39,818] INFO Accepted socket connection from /127.0.0.1:33820 (org.apache.zookeeper.server.NIOServerCnxnFactory)
[2018-07-02 00:19:39,821] INFO Client attempting to establish new session at /127.0.0.1:33820 (org.apache.zookeeper.server.ZooKeeperServer)
[2018-07-02 00:19:39,822] INFO Established session 0x16456fc6dc80003 with negotiated timeout 30000 for client /127.0.0.1:33820 (org.apache.zookeeper.server.ZooKeeperServer)
Topic:metadata	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: metadata	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
Topic:reviews	PartitionCount:1	ReplicationFactor:1	Configs:
	Topic: reviews	Partition: 0	Leader: 0	Replicas: 0	Isr: 0
[2018-07-02 00:19:40,075] INFO Processed session termination for sessionid: 0x16456fc6dc80003 (org.apache.zookeeper.server.PrepRequestProcessor)
[2018-07-02 00:19:40,077] INFO Closed socket connection for client /127.0.0.1:33820 which had sessionid 0x16456fc6dc80003 (org.apache.zookeeper.server.NIOServerCnxn)
```


7. Launch the Kafka standalone application:


== SETTING UP THE DSE GRAPH SCHEMA

Provided in the project's `resources` subdirectory is a sample DSE Graph schema script `AmazonDSEMaterializedCustomID.groovy`, used for loading data into a DSE Graph instance called `graph_stress`. Let's go ahead and create this Graph schema through the Gremlin shell with the following script:
```

```




== RUNNING THE SPARK APPLICATION

Now that the overhead involved with setting up our Kafka topics and DSE Graph instance is finished, we return to the Spark application itself, for loading data into our craeted DSE Graph instance 'graph_stress`.
